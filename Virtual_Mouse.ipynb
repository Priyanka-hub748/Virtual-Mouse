{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdad81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "mode = 'gesture'  # default mode\n",
    "\n",
    "scroll_buffer = deque(maxlen=5)\n",
    "scroll_up_queue = deque(maxlen=3)  # stores last 3 gesture states\n",
    "\n",
    "\n",
    "# ✅ Add these for camera + gesture timing\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_y_index = 0\n",
    "prev_y_middle = 0\n",
    "prev_right_click_time = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b26973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finger_status(hand_landmarks):\n",
    "    finger_tips = [8, 12, 16, 20]\n",
    "    finger_fold_status = []\n",
    "\n",
    "    for tip in finger_tips:\n",
    "        if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y:\n",
    "            finger_fold_status.append(1)\n",
    "        else:\n",
    "            finger_fold_status.append(0)\n",
    "\n",
    "    thumb_tip = hand_landmarks.landmark[4]\n",
    "    thumb_ip = hand_landmarks.landmark[3]\n",
    "    if thumb_tip.x < thumb_ip.x:\n",
    "        thumb = 1\n",
    "    else:\n",
    "        thumb = 0\n",
    "\n",
    "    return [thumb] + finger_fold_status\n",
    "\n",
    "\n",
    "def listen_voice_command():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for voice command...\")\n",
    "        audio = recognizer.listen(source, phrase_time_limit=3)\n",
    "\n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            print(\"Heard:\", command)\n",
    "            return command\n",
    "        except:\n",
    "            print(\"Could not recognize.\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "def calculate_scroll_direction(buffer):\n",
    "    if len(buffer) < 5:\n",
    "        return 0\n",
    "    y_vals = [pt[1] for pt in buffer]\n",
    "    diff = y_vals[-1] - y_vals[0]\n",
    "    if abs(diff) < 0.02:\n",
    "        return 0\n",
    "    return -1 if diff < 0 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a204dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_click_time = 0\n",
    "click_delay = 0.7  # seconds between clicks to avoid repeats\n",
    "\n",
    "mode_switch_time = 0\n",
    "mode_switch_delay = 1.5  # seconds between mode switches to avoid accidental toggling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c1ca96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for voice command...\n",
      "Heard: switch\n",
      "Listening for voice command...\n",
      "Heard: switch to gesture\n",
      "Listening for voice command...\n",
      "Heard: scroll up\n",
      "Listening for voice command...\n",
      "Heard: scroll up\n",
      "Listening for voice command...\n",
      "Could not recognize.\n",
      "Listening for voice command...\n",
      "Heard: scroll\n",
      "Listening for voice command...\n",
      "Heard: scroll up\n",
      "Listening for voice command...\n",
      "Heard: move left\n",
      "Listening for voice command...\n",
      "Heard: right click\n",
      "Listening for voice command...\n",
      "Heard: move right\n",
      "Listening for voice command...\n",
      "Could not recognize.\n",
      "Listening for voice command...\n",
      "Heard: move\n",
      "Listening for voice command...\n",
      "Heard: move up\n",
      "Listening for voice command...\n",
      "Heard: left click\n",
      "Listening for voice command...\n",
      "Could not recognize.\n",
      "Listening for voice command...\n",
      "Heard: iske liye kyon hai iske liye\n",
      "Listening for voice command...\n",
      "Could not recognize.\n",
      "Listening for voice command...\n",
      "Heard: quit\n",
      "Quitting program...\n"
     ]
    }
   ],
   "source": [
    "# 4th cell — GESTURE CONTROL ONLY (No imports here)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_right_click_time = 0\n",
    "prev_left_click_time = 0\n",
    "prev_scroll_time = 0\n",
    "scroll_cooldown = 1\n",
    "\n",
    "while True:\n",
    "    if mode == 'voice':\n",
    "        cap.release()\n",
    "        command = listen_voice_command()\n",
    "\n",
    "        # ——— Add quit command here ———\n",
    "        if 'quit' in command:\n",
    "            print(\"Quitting program...\")\n",
    "            break\n",
    "\n",
    "        elif 'left click' in command:\n",
    "            pyautogui.click()\n",
    "        elif 'right click' in command:\n",
    "            pyautogui.rightClick()\n",
    "        elif 'scroll up' in command:\n",
    "            pyautogui.scroll(300)\n",
    "        elif 'scroll down' in command:\n",
    "            pyautogui.scroll(-300)\n",
    "        elif 'volume up' in command:\n",
    "            pyautogui.press('volumeup')\n",
    "        elif 'volume down' in command:\n",
    "            pyautogui.press('volumedown')\n",
    "        elif 'move up' in command:\n",
    "            pyautogui.moveRel(0, -50)\n",
    "        elif 'move down' in command:\n",
    "            pyautogui.moveRel(0, 50)\n",
    "        elif 'move left' in command:\n",
    "            pyautogui.moveRel(-100, 0)\n",
    "        elif 'move right' in command:\n",
    "            pyautogui.moveRel(100, 0)\n",
    "\n",
    "        # Switch back to gesture mode\n",
    "        elif 'switch to gesture' in command:\n",
    "            mode = 'gesture'\n",
    "            cap = cv2.VideoCapture(0)\n",
    "\n",
    "        continue\n",
    "    # Gesture Mode\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(img_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmark in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(img, hand_landmark, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            lm_list = []\n",
    "            for id, lm in enumerate(hand_landmark.landmark):\n",
    "                h, w, _ = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lm_list.append((cx, cy))\n",
    "\n",
    "            if lm_list:\n",
    "                fingers = []\n",
    "                # Thumb\n",
    "                fingers.append(1 if lm_list[4][0] > lm_list[3][0] else 0)\n",
    "                # Fingers\n",
    "                for tip in [8, 12, 16, 20]:\n",
    "                    fingers.append(1 if lm_list[tip][1] < lm_list[tip - 2][1] else 0)\n",
    "\n",
    "                # GESTURE: Mouse Move\n",
    "                if fingers == [0, 1, 0, 0, 0]:\n",
    "                 x, y = lm_list[8]\n",
    "                 screen_x = np.interp(x, [0, w], [0, screen_width])\n",
    "                 screen_y = np.interp(y, [0, h], [0, screen_height])\n",
    "                 pyautogui.moveTo(screen_x, screen_y)\n",
    "                 cv2.putText(img, \"Mouse Move\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "                # GESTURE: Left Click (Index + Thumb pinch)\n",
    "                elif fingers == [0, 1, 1, 1, 0]:  # Left Click\n",
    "                 current_time = time.time()\n",
    "                 if current_time - prev_left_click_time > 1:\n",
    "                  pyautogui.click()\n",
    "                  prev_left_click_time = current_time\n",
    "                  suppress_mouse_move = True\n",
    "                  cv2.putText(img, \"Left Click\", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                # GESTURE: Right Click (Index + Middle up)\n",
    "                elif fingers == [0, 1, 1, 0, 0]:\n",
    "                 current_time = time.time()\n",
    "                 if current_time - prev_right_click_time > 1:\n",
    "                  pyautogui.rightClick()\n",
    "                  prev_right_click_time = current_time\n",
    "                  suppress_mouse_move = True\n",
    "                  cv2.putText(img, \"Right Click\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "               # GESTURE: Scroll Up — 4 fingers up (thumb down)\n",
    "                elif fingers == [0, 1, 1, 1, 1]:\n",
    "                 current_time = time.time()\n",
    "                 if current_time - prev_scroll_time > scroll_cooldown:\n",
    "                  pyautogui.scroll(300)  # Scroll Up\n",
    "                  prev_scroll_time = current_time\n",
    "                  cv2.putText(img, \"Scroll Up\", (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 100), 2)\n",
    "\n",
    "               # GESTURE: Scroll Down — all 5 fingers up\n",
    "                elif fingers == [1, 1, 1, 1, 1]:\n",
    "                 current_time = time.time()\n",
    "                 if current_time - prev_scroll_time > scroll_cooldown:\n",
    "                  pyautogui.scroll(-300)  # Scroll Down\n",
    "                  prev_scroll_time = current_time\n",
    "                  cv2.putText(img, \"Scroll Down\", (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 100), 2)\n",
    "\n",
    "                # GESTURE: Volume Up — thumb only\n",
    "                elif fingers == [1, 0, 0, 0, 0]:\n",
    "                    pyautogui.press('volumeup')\n",
    "                    time.sleep(0.2)\n",
    "                    cv2.putText(img, \"Volume Up\", (10, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "                # GESTURE: Volume Down — all down (back palm must be ensured via hand orientation)\n",
    "                elif fingers == [0, 0, 0, 0, 0]:\n",
    "                    pyautogui.press('volumedown')\n",
    "                    time.sleep(0.3)\n",
    "                    cv2.putText(img, \"Volume Down\", (10, 230), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "                # GESTURE: Switch to Voice — YoYo gesture (thumb + index + pinky)\n",
    "                elif fingers == [1, 1, 0, 0, 1]:\n",
    "                    mode = 'voice'\n",
    "                    cv2.putText(img, \"Switched to Voice\", (10, 260), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    time.sleep(1)\n",
    "    cv2.imshow(\"Gesture Mode\", img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
